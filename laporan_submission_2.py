# -*- coding: utf-8 -*-
"""laporan_submission_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iYercLoiDMMjIC5v88knVhlCjZ_HFjhu

#Laporan Proyek Machine Learning - Putu Gio Satria Adinata

##Data Loading

###Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
#Impor Library
import kagglehub
import shutil
import os
import zipfile
import pandas as pd
import numpy as np
import seaborn as sns
from scipy import stats
from ast import literal_eval # Untuk mengubah string list menjadi list aktual
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dot, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import layers
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import missingno as msno # Untuk visualisasi missing values
from wordcloud import WordCloud

# %matplotlib inline

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.utils import shuffle
from scipy.sparse import csr_matrix
from sklearn.model_selection import train_test_split

print("Semua Library berhasil diimpor.")

"""###Download Dataset
Peneliti mengambil dataset dari Kaggle yakni Spotify dataset: A Comprehensive Collection of Spotify Tracks Across Various Genres yang diupload oleh Gati Ambaliya. Datasetnya dapat dilihat pada tautan ini https://www.kaggle.com/datasets/ambaliyagati/spotify-dataset-for-playing-around-with-sql

"""

import kagglehub

# Download latest version
df = kagglehub.dataset_download("ambaliyagati/spotify-dataset-for-playing-around-with-sql")

print("Path to dataset files:", df)

df = pd.read_csv('/kaggle/input/spotify-dataset-for-playing-around-with-sql/spotify_tracks.csv')

"""###Reading Data

Pada tahap ini peneliti akan membaca dataset yang sudah kita ambil dari Kaggle
"""

df.head()

print('Jumlah data track \t\t: ', len(df.name.unique()))
print('Jumlah data artist \t\t: ', len(df.artists.unique()))
print('Jumlah data album \t\t: ', len(df.album.unique()))
print('Jumlah data popularity \t\t: ', len(df.popularity.unique()))
print('Jumlah data genre \t\t: ', len(df.genre.unique()))

"""Berdasarkan varibel-variabel dataset di atas, peneliti cukup mengambil variabel sesuai kebutuhan analisis dan pelatihan model pada proyek ini yakni name, artists, album, popularity, genre

##Exploratory Data Analysis (EDA)

###Univariate Analysis

Analisis univariat merupakan bagian fundamental dari Analisis Data Eksploratori (EDA), di mana Anda menganalisis satu variabel pada satu waktu. Tujuannya utama adalah untuk memahami distribusi nilai, mengidentifikasi kecenderungan pusat, mengukur penyebaran, dan mendeteksi anomali atau pola dalam variabel tunggal tersebut, tanpa mempertimbangkan hubungan dengan variabel lain.

Kode di bawah melakukan dua visualisasi data terkait popularitas lagu dan artis menggunakan pustaka Matplotlib dan Seaborn. Pertama, kode menghasilkan histogram distribusi skor popularitas lagu dari seluruh dataset. Histogram ini menunjukkan seberapa sering rentang skor popularitas tertentu muncul, dengan tambahan kurva KDE (Kernel Density Estimate) untuk memperhalus gambaran distribusi. Sumbu X merepresentasikan skor popularitas, dan sumbu Y adalah frekuensi atau jumlah lagu.
"""

# Popularity Distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['popularity'], kde=True, bins=20)
plt.title('Distribusi Popularitas Lagu')
plt.xlabel('Popularitas')
plt.ylabel('Frekuensi')
plt.show()

top_artists = df.sort_values(by='popularity', ascending=False).head(10)
plt.figure(figsize=(14, 8))
plot = sns.barplot(x='popularity', y='artists', data=top_artists, palette='pastel')
plt.title('Top 10 Artist dengan Popularity Tertinggi')
plt.xlabel('Popularity')
plt.ylabel('Artist')
for i in plot.patches:
    plot.text(i.get_width() + 0.2, i.get_y() + i.get_height()/2, str(round(i.get_width(), 2)), fontsize=10, color='black', ha='left', va='center')
plt.show()

# Top 10 Genres by Number of Songs
top_genres = df['genre'].value_counts().head(10)
plt.figure(figsize=(12, 8))
sns.barplot(x=top_genres.values, y=top_genres.index, palette='viridis')
plt.title('Top 10 Genres by Number of Songs')
plt.xlabel('Number of Songs')
plt.ylabel('Genre')
plt.show()

# Distribution of Song Duration
plt.figure(figsize=(10, 6))
sns.histplot(df['duration_ms'] / 1000, bins=30, kde=True, color='green')
plt.title('Distribution of Song Duration')
plt.xlabel('Duration (seconds)')
plt.ylabel('Frequency')
plt.show()

"""Dari gambar di atas, sebagian besar lagu memiliki durasi yang relatif pendek hingga sedang. Puncak tertinggi (modus) dari distribusi ini tampaknya berada di sekitar 200-250 detik (sekitar 3-4 menit), yang merupakan durasi umum untuk lagu-lagu populer.

###Bivariate Analysis

Kode di bawah digunakan untuk melakukan analisis bivariat dengan membuat scatter plot yang bertujuan untuk melihat hubungan antara dua variabel numerik: durasi lagu dan popularitas lagu. Setiap titik pada plot mewakili satu lagu. Posisi horizontal titik ditentukan oleh durasi lagu (dalam detik, setelah dikonversi dari milidetik), dan posisi vertikalnya ditentukan oleh skor popularitas lagu tersebut.
"""

# Scatter plot of Popularity vs Duration
plt.figure(figsize=(10, 6))
sns.scatterplot(x=df['duration_ms'] / 1000, y=df['popularity'], hue=df['genre'], palette='deep', alpha=0.7)
plt.title('Popularity vs Duration of Songs')
plt.xlabel('Duration (seconds)')
plt.ylabel('Popularity')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""Sebagian besar lagu terkonsentrasi di area kiri plot, menunjukkan bahwa mayoritas lagu memiliki durasi yang relatif pendek (umumnya di bawah sekitar 500-750 detik atau sekitar 8-12 menit, dengan kepadatan tertinggi pada durasi yang lebih pendek lagi, mungkin sekitar 150-300 detik atau 2.5-5 menit). Popularitas lagu-lagu dalam klaster utama ini bervariasi dari rendah hingga sangat tinggi.

##Data Preparation

###Mengambil Fitur Sesuai Kebutuhan

Pada tahap ini, kita hanya mengambil beberapa fitur atau kolom dari variabel movies sesuai kebutuhan analsis pengolahan data yakni id, name, genre, artists, album, popularity
"""

# Menghapus beberapa fitur dari DataFrame
features_to_drop = ['duration_ms', 'explicit']
df_clean = df.drop(columns=features_to_drop)

# Memeriksa DataFrame setelah fitur dihapus
df = df_clean
df.head()

df_clean = df[['name', 'artists', 'genre']].drop_duplicates(subset='name', keep='first')

"""###Menangani Nilai Kosong (Missing Value)

Pada tahap ini peneliti akan melakukan pengecekan nilai kosong pada variabel dataset
"""

df.isnull().sum()

"""Dari hasil di atas, semua kolom tidak memiliki nilai kosong dan bisa melanjutkan ke tahap berikutnya

###Menangani Duplikat Data

Pada tahap ini peneliti akan melakukan pengecekan duplikat data pada variabel dataset
"""

df.duplicated().sum()

"""Dari hasil di atas, semua kolom tidak memiliki data yang terduplikat dan bisa melanjutkan ke tahap berikutnya

###Data Preprocessing

####Mengurutkan data berdasarkan Popularity
"""

unique_popularity = df['popularity'].unique()
unique_popularity.sort()
unique_popularity

"""####Menampilkan nilai unik dari kolom 'genre'"""

with pd.option_context('display.max_rows', None, 'display.max_columns', None):
  print(df['genre'].value_counts())

"""##Content Based Filtering (CBF)

###A. Data Preparation

Tahap ini fokus pada pengolahan fitur atau atribut dari item itu sendiri. Proses utamanya meliputi pembersihan data teks, ekstraksi fitur (seperti TF-IDF untuk mengubah teks menjadi vektor numerik), dan penggabungan berbagai fitur menjadi satu profil komprehensif untuk setiap item.
"""

# Membuat instance dari TfidfVectorizer
tfidf = TfidfVectorizer()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfidf.fit_transform(df['genre'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Kode di atas menginisialisasi sebuah TfidfVectorizer yang kemudian digunakan untuk mengubah data tekstual dari kolom 'genre' dalam DataFrame df menjadi representasi numerik dalam bentuk matriks TF-IDF. Proses fit_transform pertama-tama mempelajari seluruh kosakata unik dari semua genre dan menghitung bobot IDF-nya, lalu mengubah setiap teks genre menjadi vektor skor TF-IDF. Hasilnya, tfidf_matrix adalah matriks di mana baris mewakili lagu dan kolom mewakili kata unik dari kosakata genre, dengan nilai sel berupa skor TF-IDF. Perintah tfidf_matrix.shape akan menunjukkan jumlah lagu dan jumlah kata unik (fitur) yang berhasil diekstrak dari data genre tersebut. Matriks ini kemudian siap digunakan untuk langkah selanjutnya dalam CBF, seperti menghitung kemiripan antar lagu berdasarkan profil genre mereka."""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Kode tfidf_matrix.todense() mengubah matriks TF-IDF, yang awalnya disimpan dalam format sparse (hemat memori dengan hanya menyimpan nilai non-nol), menjadi format dense (matriks padat). Dalam matriks padat ini, semua elemen, termasuk skor TF-IDF nol, akan disimpan secara eksplisit."""

# Membuat dataframe untuk tf-idf matrix
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=df.name
)

"""###B. Modeling

Pada tahap ini, "model" seringkali berupa representasi item dalam bentuk vektor dan matriks kemiripan antar item. Setelah item diubah menjadi vektor (misalnya, vektor TF-IDF), kemiripan antar item dihitung (umumnya menggunakan cosine similarity). Matriks kemiripan inilah yang menjadi dasar untuk membuat rekomendasi.
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(
    cosine_sim,
    index=df['name'],
    columns=df['name']
)

# Melihat similarity matrix pada setiap lagu
cosine_sim_df.sample(25, axis=1).sample(25, axis=0)

"""Kode di atas melakukan tahap inti dalam pemodelan Content-Based Filtering dengan menghitung matriks kemiripan (similarity matrix) antar semua lagu menggunakan metrik cosine similarity pada matriks TF-IDF yang sudah ada."""

def song_recommendations(nama_lagu, similarity_data=cosine_sim_df, items=df[['name', 'genre']], k=5):
    index = similarity_data.loc[:,nama_lagu].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_lagu agar nama lagu yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_lagu, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Fungsi song_recommendations ini bertujuan untuk menghasilkan daftar rekomendasi lagu berdasarkan kemiripan konten dengan lagu yang diberikan sebagai input (nama_lagu)

###C. Pengujian Sistem Rekomendasi

Pada tahap ini pengujian dilakukan dengan memberikan satu item sebagai input Sistem kemudian akan menggunakan matriks kemiripan untuk menemukan item-item lain yang paling mirip dengan input tersebut dan menampilkannya sebagai rekomendasi.
"""

df['artists']

# Menampilkan daftar lagu dengan artis John Denver
spotify_songs = df[df['artists'] == 'John Denver'].head()
spotify_songs

# Mendapatkan rekomendasi lagu yang mirip dengan lagu Rocky Mountain High
song_recommendations('Rocky Mountain High')

"""Dari hasil rekomendasi di atas, TF-IDF fokus pada frekuensi kata. Jika kata "rock" (atau kata umum lainnya) muncul dalam deskripsi genre 'Rocky Mountain High' dan juga sering muncul dalam deskripsi genre lagu-lagu 'j-rock' dengan cara tertentu yang membuatnya signifikan menurut perhitungan TF-IDF, ini bisa menyebabkan kedekatan dalam ruang vektor.

###D. Evaluation

Pada tahap ini bertujuan menilai relevansi dan kualitas rekomendasi.
"""

def get_recommended_song_names(input_song, similarity_data, k):
    """Mengembalikan daftar nama lagu hasil rekomendasi."""
    recommendations = song_recommendations(input_song, similarity_data, df[['name', 'genre']], k)
    return recommendations['name'].tolist()

def evaluate_recommendations(input_song, similarity_data=cosine_sim_df, k=5):
    recommended_songs = get_recommended_song_names(input_song, similarity_data, k)
    scores = similarity_data.loc[input_song, recommended_songs]
    return scores.mean() if not scores.empty else 0.0

def evaluate_precision(input_song, ground_truth, similarity_data=cosine_sim_df, k=5):
    """Menghitung precision rekomendasi berdasarkan kecocokan dengan ground truth relevan."""
    recommended_songs = get_recommended_song_names(input_song, similarity_data, k)
    relevant_songs = set(ground_truth.get(input_song, []))
    true_positives = sum(song in relevant_songs for song in recommended_songs)
    return (true_positives / k) * 100 if k > 0 else 0.0

input_song = 'Rocky Mountain High'

ground_truth = {
    'Rocky Mountain High': ['hikikomori rock', 'Jibun ROCK', 'Prove - Japanese Version', 'Ceria', "Fallin' In Love" ]
}

avg_sim = evaluate_recommendations(input_song, k=5)
precision = evaluate_precision(input_song, ground_truth, k=5)

print(f"Average similarity of recommended songs to '{input_song}': {avg_sim:.4f}")
print(f"Precision for recommendations of '{input_song}': {precision:.2f}%")

"""Persentase presisi (0-100%). Nilai 100% berarti semua lagu yang direkomendasikan ada dalam daftar ground_truth yang relevan.

Meskipun skor sempurna terlihat bagus, dalam sistem rekomendasi nyata dengan dataset besar dan beragam, mencapai 100% presisi dan 1.0 rata-rata similaritas secara konsisten untuk semua input biasanya sulit. Hasil ini bisa menjadi indikasi bahwa untuk kasus spesifik 'Rocky Mountain High' dengan ground_truth yang Anda berikan dan fitur yang digunakan, sistem bekerja dengan sangat baik, atau ada kondisi tertentu dalam data Anda (seperti kesamaan fitur yang tinggi antar lagu tersebut) yang mengarah ke hasil ini. Maka dari itu, dilakukan pendekatan Collaborative Filtering untuk mengetahui hasil yang lebih lanjut.

##Collaborative Filtering

###A. Data Preparation

Tahap ini fokus pada pengolahan data interaksi antara pengguna dan item. Langkah utamanya adalah membuat matriks interaksi pengguna-item, memetakan ID pengguna dan item ke format numerik, dan kadang melakukan normalisasi (misalnya pada rating).
"""

# Create user and song ID mappings
user_ids = df['artists'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

song_ids = df['name'].unique().tolist()
song_to_song_encoded = {x: i for i, x in enumerate(song_ids)}
song_encoded_to_song = {i: x for i, x in enumerate(song_ids)}

"""Kode di atas melakukan pra-pemrosesan data dengan membuat sistem pemetaan ID dua arah untuk artis (yang di sini tampaknya diperlakukan sebagai proxy untuk pengguna) dan lagu. Untuk setiap entitas (artis dan lagu), kode ini pertama-tama mengekstrak daftar nama unik dari DataFrame."""

# Map user and song IDs to encoded values
df['user_encoded'] = df['artists'].map(user_to_user_encoded)
df['song_encoded'] = df['name'].map(song_to_song_encoded)

num_users = len(user_to_user_encoded)
num_songs = len(song_to_song_encoded)

"""Kode di atas melanjutkan proses pra-pemrosesan data dengan menerapkan pemetaan ID yang telah dibuat sebelumnya ke DataFrame utama dan menghitung jumlah total pengguna (artis) dan lagu unik."""

# Scale the popularity score
min_popularity = df['popularity'].min()
max_popularity = df['popularity'].max()
df['popularity_scaled'] = df['popularity'].apply(lambda x: (x - min_popularity) / (max_popularity - min_popularity)).values

"""Kode di atas melakukan normalisasi Min-Max pada kolom 'popularity' di DataFrame df dan menyimpan hasilnya dalam kolom baru bernama 'popularity_scaled'.  rescaling ini mengubah rentang skor popularitas asli menjadi rentang baru antara 0 dan 1."""

# Shuffle the data
df = shuffle(df, random_state=42)

# Split data into training and validation sets
train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)

"""Kode di atas pertama-tama mengacak urutan seluruh baris data dalam DataFrame df menggunakan fungsi shuffle untuk memastikan tidak ada bias karena urutan data asli dan untuk mendukung pembagian data yang representatif. Penggunaan random_state=42 menjamin bahwa hasil pengacakan ini konsisten setiap kali kode dijalankan. Setelah diacak, data tersebut kemudian dibagi menjadi dua bagian menggunakan train_test_split: 80% data dialokasikan sebagai train_data yang akan digunakan untuk melatih model, dan 20% sisanya dialokasikan sebagai val_data yang akan digunakan untuk menguji dan memvalidasi performa model pada data yang belum pernah dilihat sebelumnya, juga dengan random_state untuk reprodusibilitas.

###B. Modeling

Pada tahap ini, Modeling pada CF bertujuan untuk mempelajari pola preferensi pengguna dari data interaksi kolektif.

Kode di bawah mendefinisikan sebuah model jaringan saraf tiruan (neural network) menggunakan Keras (bagian dari TensorFlow) yang disebut RecommenderNet. Model ini dirancang untuk sistem rekomendasi, kemungkinan besar menggunakan pendekatan collaborative filtering berbasis matrix factorization dengan tambahan bias dan layer Dense.
"""

# Keras Model Definition
class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_songs, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_songs = num_songs
        self.embedding_size = embedding_size
        self.user_embedding = tf.keras.layers.Embedding(
            input_dim=num_users,
            output_dim=embedding_size,
            name="user_embedding",
        )
        self.user_bias = tf.keras.layers.Embedding(input_dim=num_users, output_dim=1, name="user_bias")
        self.song_embedding = tf.keras.layers.Embedding(
            input_dim=num_songs,
            output_dim=embedding_size,
            name="song_embedding",
        )
        self.song_bias = tf.keras.layers.Embedding(input_dim=num_songs, output_dim=1, name="song_bias")
        # Define the Dense layer here in the constructor
        self.dense_layer = tf.keras.layers.Dense(1, activation='sigmoid')


    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        song_vector = self.song_embedding(inputs[:, 1])
        song_bias = self.song_bias(inputs[:, 1])
        dot_product = tf.tensordot(user_vector, song_vector, 2)
        # Add biases
        x = dot_product + user_bias + song_bias
        x = tf.nn.sigmoid(x)
        # Call the pre-defined Dense layer
        x = self.dense_layer(x)
        return x

embedding_size = 50
model = RecommenderNet(num_users, num_songs, embedding_size)

"""Setelah model (RecommenderNet dalam kasus ini) didefinisikan dan diinisialisasi, langkah selanjutnya sebelum model tersebut dapat dilatih adalah mengompilasinya menggunakan metode model.compile()."""

# Compile the model
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Prepare training and validation data
X_train = train_data[['user_encoded', 'song_encoded']].values
y_train = train_data['popularity_scaled'].values
X_val = val_data[['user_encoded', 'song_encoded']].values
y_val = val_data['popularity_scaled'].values

"""Kode di atas mempersiapkan data input (fitur) dan output (target/label) yang akan digunakan untuk melatih dan memvalidasi model RecommenderNet. Untuk data pelatihan, X_train dibentuk dari kolom ID pengguna dan ID lagu yang sudah di-encode, sementara y_train adalah skor popularitas yang sudah dinormalisasi. Proses serupa dilakukan untuk data validasi, menghasilkan X_val dan y_val."""

# Train the model
history = model.fit(
    x=X_train,
    y=y_train,
    batch_size=8,
    epochs=50,
    verbose=1,
    validation_data=(X_val, y_val)
)

"""Dapat dilihat, hasil pelatihan memperoleh nilai mean_absolute_error: 0.5295 dan root_mean_squared_error: 0.0669

###C. Evaluation

Pada tahap ini peneliti akan melakukan visualisasi metrik seperti Mean Absolute Error (MAE) dan Root Mean Squared Error (RMSE). Kedua metrik ini sangat penting dalam mengevaluasi kinerja model prediksi. Kedua metrik ini memberikan informasi tentang seberapa baik model dapat memprediksi nilai aktual, dan visualisasi dapat membantu dalam memahami perbandingan antara keduanya serta tren kesalahan dari waktu ke waktu.
"""

# Plot training and validation loss and RMSE
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')

plt.subplot(1, 2, 2)
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('Model RMSE')
plt.ylabel('RMSE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')

plt.tight_layout()
plt.show()

"""Insight:
<br>
1. Model Belajar dengan Baik pada Data Pelatihan, dimana penurunan nilai loss dan RMSE pada set pelatihan menunjukkan bahwa model berhasil mempelajari pola-pola yang ada dalam data pelatihan.
2. Overfitting Terjadi, dimana perbedaan yang signifikan dan meningkat antara kinerja pada data pelatihan dan data validasi (di mana kinerja pada data pelatihan terus membaik sementara pada data validasi stagnan atau memburuk) adalah gejala klasik dari overfitting. Ini berarti model Anda mulai "menghafal" data pelatihan, termasuk noise di dalamnya, daripada mempelajari pola umum yang bisa digeneralisasi ke data baru. Akibatnya, performa model pada data yang belum pernah dilihat (data validasi) tidak sebaik performanya pada data pelatihan.
3. Epoch Optimal, dimana titik di mana kurva validasi mulai stagnan atau naik (sekitar epoch 5-15 dalam kasus ini) bisa dianggap sebagai perkiraan jumlah epoch optimal untuk pelatihan sebelum overfitting menjadi terlalu parah. Melatih lebih lama dari titik ini mungkin tidak meningkatkan kemampuan generalisasi model.
"""

def cf_recommendations(artist_name, n_recommendations=10):
    artist_encoded = user_to_user_encoded.get(artist_name)
    if artist_encoded is None:
        print(f"Artist '{artist_name}' not found.")
        return pd.DataFrame()

    artist_songs = df[df['artists'] == artist_name]
    artist_songs_encoded = artist_songs['song_encoded'].tolist()

    # Cari lagu yang belum pernah didengar
    all_song_encoded = list(song_encoded_to_song.keys())
    unseen_songs_encoded = [
        song_id for song_id in all_song_encoded if song_id not in artist_songs_encoded
    ]

    artist_input = np.full((len(unseen_songs_encoded), 1), artist_encoded)
    unseen_songs_input = np.array(unseen_songs_encoded).reshape(-1, 1)
    prediction_input = np.concatenate([artist_input, unseen_songs_input], axis=1)

    # Prediksi popularitas
    predicted_popularities_scaled = model.predict(prediction_input).flatten()

    # Dataframe hasil
    unseen_songs_df = pd.DataFrame({
        'song_encoded': unseen_songs_encoded,
        'predicted_popularity_scaled': predicted_popularities_scaled
    })

    unseen_songs_df['name'] = unseen_songs_df['song_encoded'].map(song_encoded_to_song)
    df_clean = df[['name', 'artists', 'genre']].drop_duplicates(subset='name', keep='first')

    # Gabungkan info lagu ke hasil prediksi
    recommendation_df = unseen_songs_df.merge(
        df_clean,
        on='name',
        how='left'
        )
    # Ambil top N
    top_recommendations = recommendation_df.sort_values(
        by='predicted_popularity_scaled',
        ascending=False
    ).head(n_recommendations)

    return top_recommendations[['name', 'artists', 'genre', 'predicted_popularity_scaled']]

artist_to_recommend = 'Morgan Wallen'

cf_recommendations_df = cf_recommendations(artist_to_recommend, n_recommendations=10)

if not cf_recommendations_df.empty:
    print(f"\nRekomendasi 10 Lagu Terbaik untuk: {artist_to_recommend}")
    print('----' * 20)
    for i, row in enumerate(cf_recommendations_df.itertuples(), start=1):
        print(f"No         : {i}")
        print(f"Nama Lagu  : {row.name}")
        print(f"Artist     : {row.artists}")
        print(f"Genre      : {row.genre}")
        print(f"Skor Prediksi Popularitas: {row.predicted_popularity_scaled:.4f}")
        print('----' * 20)
artist_to_recommend = 'Morgan Wallen'

cf_recommendations_df = cf_recommendations(artist_to_recommend, n_recommendations=10)

final_train_rmse = history.history['root_mean_squared_error'][-1]
final_val_rmse = history.history['val_root_mean_squared_error'][-1]

print(f"Final Training RMSE: {final_train_rmse:.4f}")
print(f"Final Validation RMSE: {final_val_rmse:.4f}")

"""Secara umum, nilai RMSE yang lebih rendah menunjukkan performa model yang lebih baik karena prediksinya lebih dekat dengan nilai aktual. Perbandingan antara Training RMSE dan Validation RMSE juga bisa memberikan indikasi apakah model mengalami overfitting (jika Training RMSE jauh lebih rendah dari Validation RMSE)."""